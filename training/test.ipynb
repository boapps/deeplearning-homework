{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegmentation_project\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmelytanulo-buvarok\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVOCDataset\u001b[39;00m(Dataset):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_dir, mask_dir, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1266\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[0;32m   1262\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[1;32m-> 1266\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\wandb\\analytics\\sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1252\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m   1251\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:844\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    842\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[1;32m--> 844\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "import numpy as np\n",
    "import torchvision.models.segmentation as seg_models\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"segmentation_project\", entity=\"melytanulo-buvarok\")\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.images = [f for f in sorted(os.listdir(image_dir)) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png').replace('.jpeg', '.png'))\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "         \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        \n",
    "        mask = np.array(mask, dtype=np.int64)\n",
    "        return image, torch.from_numpy(mask)\n",
    "\n",
    "def create_data_loaders(image_dir, mask_dir, batch_size=8, val_split=0.2, num_workers=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), interpolation=Image.NEAREST)\n",
    "    ])\n",
    "\n",
    "    full_dataset = VOCDataset(image_dir, mask_dir, transform=transform, mask_transform=mask_transform)\n",
    "    \n",
    "    total_size = len(full_dataset)\n",
    "    val_size = int(val_split * total_size)\n",
    "    train_size = total_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    print(f\"Total dataset size: {total_size}\")\n",
    "    print(f\"Training set size: {train_size}\")\n",
    "    print(f\"Validation set size: {val_size}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.model = seg_models.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)\n",
    "        self.jaccard = JaccardIndex(task='multiclass', num_classes=num_classes, ignore_index=255)\n",
    "        self.mean_iou = MeanIoU(num_classes=num_classes) # no ignore_index :(\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, masks, ignore_index=255)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        # Log to WandB\n",
    "        wandb.log({\"train_loss\": loss})\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, masks, ignore_index=255)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        jaccard = self.jaccard(preds, masks)\n",
    "        mean_iou = self.mean_iou(preds, masks)\n",
    "        \n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_jaccard\", jaccard)\n",
    "        self.log(\"val_mean_iou\", mean_iou)\n",
    "        \n",
    "        # Log to WandB\n",
    "        wandb.log({\"val_loss\": loss, \"val_jaccard\": jaccard, \"val_mean_iou\": mean_iou})\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "# Initialize model and trainer\n",
    "model = UNet(num_classes=21)\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator='auto', logger=pl.loggers.WandbLogger())\n",
    "train_loader, val_loader = create_data_loaders('./img', './msk')\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Run evaluation on the validation set\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    jaccard_meter = JaccardIndex(task='multiclass', num_classes=21, ignore_index=255)\n",
    "    mean_iou_meter = MeanIoU(num_classes=21)\n",
    "    val_loss_meter = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, masks = batch\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, masks, ignore_index=255)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            val_loss_meter += loss.item()\n",
    "            jaccard_meter(preds, masks)\n",
    "            mean_iou_meter(preds, masks)\n",
    "            count += 1\n",
    "    \n",
    "    avg_val_loss = val_loss_meter / count\n",
    "    avg_jaccard = jaccard_meter.compute()\n",
    "    avg_mean_iou = mean_iou_meter.compute()\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Jaccard Index: {avg_jaccard:.4f}\")\n",
    "    print(f\"Validation Mean IoU: {avg_mean_iou:.4f}\")\n",
    "\n",
    "# Call the evaluation method\n",
    "evaluate_model(model, val_loader)\n",
    "\n",
    "# Save the model\n",
    "model_path = 'unet_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
