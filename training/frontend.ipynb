{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in c:\\users\\naang\\anaconda3\\envs\\dl_test\\lib\\site-packages (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset file at: .gradio\\flagged\\dataset2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naang\\AppData\\Local\\Temp\\ipykernel_4192\\1506363326.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../data/cnn_v2.pth', map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.segmentation import FCN_ResNet50_Weights\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation import fcn_resnet50, fcn_resnet101\n",
    "from torchvision import transforms\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "id2label = {i: str(i) for i in range(20)}\n",
    "id2label[255] = \"255\"\n",
    "label2id = {str(i): i for i in range(20)}\n",
    "label2id[\"255\"] = 255\n",
    "\n",
    "# Load the model architecture\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    checkpoint, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# Load custom trained weights using Safetensors\n",
    "from safetensors.torch import load_file\n",
    "custom_checkpoint_path = \"../data/vit/model.safetensors\"  # Path to your trained weights\n",
    "state_dict = load_file(custom_checkpoint_path)  # Load the Safetensors file\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Define the image processor\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)\n",
    "\n",
    "def model_vit(image):\n",
    "    inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1],  # (height, width)\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    return Image.fromarray((predicted_segmentation * 255).astype(np.uint8))\n",
    "\n",
    "def model_cnn1(image):\n",
    "    # Load the FCN-ResNet50 model\n",
    "    model = fcn_resnet50(weights=None, num_classes=21)  # Use `weights=None`\n",
    "    \n",
    "    # Adjust the keys of the checkpoint to match the model\n",
    "    checkpoint = torch.load('../data/cnn.pth', map_location=torch.device(\"cpu\"))\n",
    "    state_dict = {key.replace(\"model.\", \"\"): value for key, value in checkpoint.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define preprocessing steps\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Preprocess the input image\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        output = model(input_tensor)['out']\n",
    "        output = torch.nn.functional.interpolate(\n",
    "            output,\n",
    "            size=image.size[::-1],  # (height, width)\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return Image.fromarray((predicted_segmentation * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "def model_cnn2(image):\n",
    "    model = fcn_resnet101(weights=None, num_classes=21)\n",
    "    # Adjust the keys of the checkpoint to match the model\n",
    "    checkpoint = torch.load('../data/cnn_v2.pth', map_location=torch.device(\"cpu\"))\n",
    "    state_dict = {key.replace(\"model.\", \"\"): value for key, value in checkpoint.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define preprocessing steps\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Preprocess the input image\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        output = model(input_tensor)['out']\n",
    "        output = torch.nn.functional.interpolate(\n",
    "            output,\n",
    "            size=image.size[::-1],  # (height, width)\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return Image.fromarray((predicted_segmentation * 255).astype(np.uint8))\n",
    "\n",
    "def segment_image(image, model_choice):\n",
    "    if model_choice == \"MiT-B0\":\n",
    "        return model_vit(image)\n",
    "    elif model_choice == \"ResNet-50\":\n",
    "        return model_cnn1(image)\n",
    "    elif model_choice == \"ResNet-101\":\n",
    "        return model_cnn2(image)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=segment_image,\n",
    "    inputs=[gr.Image(type=\"pil\"), gr.Dropdown(choices=[\"MiT-B0\", \"ResNet-50\", \"ResNet-101\"], label=\"Select Model\")],\n",
    "    outputs=gr.Image(type=\"pil\")\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
