{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.classifier.bias', 'decode_head.batch_norm.weight', 'decode_head.batch_norm.running_mean']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\gradio\\blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\gradio\\blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\gradio\\utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\naang\\AppData\\Local\\Temp\\ipykernel_9696\\3770637230.py\", line 91, in segment_image\n",
      "    return model_cnn1(image)\n",
      "  File \"C:\\Users\\naang\\AppData\\Local\\Temp\\ipykernel_9696\\3770637230.py\", line 38, in model_cnn1\n",
      "    model.load_state_dict(torch.load('../data/cnn.pth', map_location=torch.device(\"cpu\")))\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\torch\\serialization.py\", line 997, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\torch\\serialization.py\", line 444, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"c:\\Users\\naang\\anaconda3\\envs\\DL_test\\lib\\site-packages\\torch\\serialization.py\", line 425, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/cnn.pth'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation import fcn_resnet50, fcn_resnet101\n",
    "from torchvision import transforms\n",
    "\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "id2label = {i: str(i) for i in range(20)}\n",
    "id2label[255] = \"255\"\n",
    "label2id = {str(i): i for i in range(20)}\n",
    "label2id[\"255\"] = 255\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)\n",
    "\n",
    "def model_vit(image):\n",
    "    inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    return Image.fromarray((predicted_segmentation * 255).astype(np.uint8))\n",
    "\n",
    "def model_cnn1(image):\n",
    "    model = fcn_resnet50(pretrained=False, num_classes=21)\n",
    "    model.load_state_dict(torch.load('../data/cnn.pth', map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out']\n",
    "        output = torch.nn.functional.interpolate(\n",
    "            output,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return Image.fromarray((predicted_segmentation * 255).astype(np.uint8))\n",
    "\n",
    "def model_cnn2(image):\n",
    "    model = fcn_resnet101(pretrained=False, num_classes=21)\n",
    "    model.load_state_dict(torch.load('../data/cnn_v2.pth', map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out']\n",
    "        output = torch.nn.functional.interpolate(\n",
    "            output,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return Image.fromarray((predicted_segmentation * 255).astype(np.uint8))\n",
    "\n",
    "def segment_image(image, model_choice):\n",
    "    if model_choice == \"MiT-B0\":\n",
    "        return model_vit(image)\n",
    "    elif model_choice == \"ResNet-50\":\n",
    "        return model_cnn1(image)\n",
    "    elif model_choice == \"ResNet-101\":\n",
    "        return model_cnn2(image)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=segment_image,\n",
    "    inputs=[gr.Image(type=\"pil\"), gr.Dropdown(choices=[\"MiT-B0\", \"ResNet-50\", \"ResNet-101\"], label=\"Select Model\")],\n",
    "    outputs=gr.Image(type=\"pil\")\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
