{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation import fcn_resnet50, fcn_resnet101\n",
    "from torchvision import transforms\n",
    "\n",
    "checkpoint = \"../data/vit\"\n",
    "id2label = {i: str(i) for i in range(20)}\n",
    "id2label[255] = \"255\"\n",
    "label2id = {str(i): i for i in range(20)}\n",
    "label2id[\"255\"] = 255\n",
    "\n",
    "fixed_color_map = {\n",
    "    0: (0, 0, 0),         # Empty\n",
    "    1: (220, 20, 60),     # Aeroplane\n",
    "    2: (255, 182, 193),   # Bicycle\n",
    "    3: (255, 105, 180),   # Bird\n",
    "    4: (255, 20, 147),    # Boat\n",
    "    5: (255, 0, 255),     # Bottle\n",
    "    6: (199, 21, 133),    # Bus\n",
    "    7: (219, 112, 147),   # Car\n",
    "    8: (255, 160, 122),   # Cat\n",
    "    9: (255, 69, 0),      # Chair\n",
    "    10: (255, 140, 0),    # Cow\n",
    "    11: (255, 215, 0),    # Diningtable\n",
    "    12: (255, 255, 0),    # Dog\n",
    "    13: (173, 255, 47),   # Horse\n",
    "    14: (0, 255, 0),      # Motorbike\n",
    "    15: (0, 128, 0),      # Person\n",
    "    16: (0, 255, 255),    # Potted Plant\n",
    "    17: (0, 191, 255),    # Sheep\n",
    "    18: (30, 144, 255),   # Sofa\n",
    "    19: (0, 0, 255),      # Train\n",
    "    20: (0, 0, 139)       # TV/Monitor\n",
    "}\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    checkpoint, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/mit-b0\", do_reduce_labels=True)\n",
    "\n",
    "def model_vit(image):\n",
    "    inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    segmented_image = np.zeros((*predicted_segmentation.shape, 3), dtype=np.uint8)\n",
    "    for label, color in fixed_color_map.items():\n",
    "        segmented_image[predicted_segmentation == label] = color\n",
    "\n",
    "    segmented_image = Image.fromarray(segmented_image)\n",
    "    return segmented_image\n",
    "\n",
    "def model_cnn1(image):\n",
    "    model = fcn_resnet50(weights=None, num_classes=21)\n",
    "    \n",
    "    checkpoint = torch.load('../data/cnn.pth', map_location=torch.device(\"cpu\"))\n",
    "    state_dict = {key.replace(\"model.\", \"\"): value for key, value in checkpoint.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out']\n",
    "        output = torch.nn.functional.interpolate(\n",
    "            output,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    segmented_image = np.zeros((*predicted_segmentation.shape, 3), dtype=np.uint8)\n",
    "    for label, color in fixed_color_map.items():\n",
    "        segmented_image[predicted_segmentation == label] = color\n",
    "\n",
    "    segmented_image = Image.fromarray(segmented_image)\n",
    "    return segmented_image\n",
    "\n",
    "def model_cnn2(image):\n",
    "    model = fcn_resnet101(weights=None, num_classes=21)\n",
    "    checkpoint = torch.load('../data/cnn_v2.pth', map_location=torch.device(\"cpu\"))\n",
    "    state_dict = {key.replace(\"model.\", \"\"): value for key, value in checkpoint.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out']\n",
    "        output = torch.nn.functional.interpolate(\n",
    "            output,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        predicted_segmentation = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    segmented_image = np.zeros((*predicted_segmentation.shape, 3), dtype=np.uint8)\n",
    "    for label, color in fixed_color_map.items():\n",
    "        segmented_image[predicted_segmentation == label] = color\n",
    "\n",
    "    segmented_image = Image.fromarray(segmented_image)\n",
    "    return segmented_image\n",
    "\n",
    "def segment_image(image, model_choice):\n",
    "    if model_choice == \"MiT-B0\":\n",
    "        return model_vit(image)\n",
    "    elif model_choice == \"ResNet-50\":\n",
    "        return model_cnn1(image)\n",
    "    elif model_choice == \"ResNet-101\":\n",
    "        return model_cnn2(image)\n",
    "\n",
    "def generate_color_legend():\n",
    "    label_names = {\n",
    "        0: \"Empty\",\n",
    "        1: \"Aeroplane\",\n",
    "        2: \"Bicycle\",\n",
    "        3: \"Bird\",\n",
    "        4: \"Boat\",\n",
    "        5: \"Bottle\",\n",
    "        6: \"Bus\",\n",
    "        7: \"Car\",\n",
    "        8: \"Cat\",\n",
    "        9: \"Chair\",\n",
    "        10: \"Cow\",\n",
    "        11: \"Diningtable\",\n",
    "        12: \"Dog\",\n",
    "        13: \"Horse\",\n",
    "        14: \"Motorbike\",\n",
    "        15: \"Person\",\n",
    "        16: \"Potted Plant\",\n",
    "        17: \"Sheep\",\n",
    "        18: \"Sofa\",\n",
    "        19: \"Train\",\n",
    "        20: \"TV/Monitor\"\n",
    "    }\n",
    "    html_content = \"<table>\"\n",
    "    for label, color in fixed_color_map.items():\n",
    "        html_content += f\"<tr><td style='background-color: rgb{color}; width: 50px; height: 20px;'></td><td>{label_names[label]}</td></tr>\"\n",
    "    html_content += \"</table>\"\n",
    "    return html_content\n",
    "\n",
    "color_legend = generate_color_legend()\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
    "            model_choice = gr.Dropdown(choices=[\"MiT-B0\", \"ResNet-50\", \"ResNet-101\"], label=\"Select Model\")\n",
    "            submit_button = gr.Button(\"Segment Image\")\n",
    "        with gr.Column():\n",
    "            segmented_image_output = gr.Image(type=\"pil\", label=\"Segmented Image\")\n",
    "            gr.HTML(color_legend)\n",
    "    \n",
    "    submit_button.click(segment_image, inputs=[image_input, model_choice], outputs=segmented_image_output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
